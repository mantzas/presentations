The Good, the Bad and the Ugly

29 Nov 2018
Tags: go, microservices, patron

Sotirios Mantziaris
Refreshment Engineer (καφετζής)

smantziaris@gmail.com
http://blog.mantziaris.eu
https://github.com/mantzas
https://gr.linkedin.com/in/mantzas
@smantziaris

* aka Go, Microservices and the Patron Framework

* The good

* 
.image img/600px-Go_Logo_Aqua.png

* why?

- easy to learn and operate
- concurrent
- fast build, test and run
- build-in testing
- build-in profiling (pprof)
- cross compilation
- single binary deployment
- a lot of high quality packages out there
- docker friendly

* go is a "cloud native" language

.image img/gopher.png

* The bad

* Microservices

There is a lot of hype around microservices and they can if used correctly solve some problems of a huge monolith.
Few people mention the problems and complexity that come with them.

They are no silver bullets.

Another good point made by Martin Fowler is: 
    
    don’t even consider microservices unless you have a system 
    that’s too complex to manage as a monolith.


* distributed computing is a hard problem

- orchestrate multiple services
- manage deployments
- monitor everything
- [[https://en.wikipedia.org/wiki/CAP_theorem][CAP Theorem]]
    ... due to the fact that network partitioning has to be tolerated 
    there are only Consistency or Availability.
- [[https://en.wikipedia.org/wiki/PACELC_theorem][PACELC theorem, CAP extension]]
    ... in case of network partitioning (P) in a distributed computer system, 
    one has to choose between availability (A) and consistency (C) (as per the CAP theorem), 
    but else (E), even when the system is running normally in the absence of partitions, 
    one has to choose between latency (L) and consistency (C).

* Fallacies of distributed computing

- The network is reliable.
- Latency is zero.
- Bandwidth is infinite.
- The network is secure.
- Topology doesn't change.
- There is one administrator.
- Transport cost is zero.
- The network is homogeneous.

* response times (RT) increases

user > Service A > Service B > Service C

where:

    RT Service A: 60ms
    RT Service B: 30ms
    RT Service C: 50ms

Total response time for the user is:

    RT Service C + RT Service B + RT Service A = 140ms

which will be generally bigger than the response time of a monolith due to network, marshal/unmarshal data etc.

* availability decreases

e.g.

    Availability Service A: 99.9%
    Availability Service B: 99.9%
    Availability Service C: 99.9%

the joint availability ([[https://en.wikipedia.org/wiki/Probability#Mathematical_treatment][probability rules]], independent events) for the user is:

    Availability Service A * Availability Service B * Availability Service C ~ 99.7%

Even if all services are 99.9% the joint availability is lower than the individual ones.

The difference is not that small! ([[https://uptime.is/][downtime calculation]])

    99.9 ~ 8h 45m 57.0s downtime
    99.7 ~ 1d 2h 17m 50.9s downtime

* coupling

[[https://en.wikipedia.org/wiki/Coupling_(computer_programming)][coupling]] is the degree of interdependence between software modules.

how can this happen?

- microservice creation based on opportunity
- shared db
- access the db directly and not through a API
- shared components, like models and business code

the above will lead to what is commonly called a "distributed monolith".

* Indications of a "distributed monolith"?

- A change to one microservice often requires changes to other microservices
- Deploying one microservice requires other microservices to be deployed at the same time
- Your microservices are overly chatty
- The same developers work across a large number of microservices
- Many of your microservices share a datastore
- Your microservices share a lot of the same code or models

* troubleshooting

In order to troubleshoot you have to look into multiple services to find the problem.

- Metrics, help send alters when conditions are met
- Metrics, help you identify the general area of a problem e.g. response time is high
- Distributed tracing, can narrow down the problem e.g. response time of the db is high on service A
- Logging, can show what the developer logs eg what happened to payment with id xxx

all the above fall under the category of *observability*.

* how can any or all of the previous be avoided?

- DDD can help, eg find bounded contexts
- event driven as the first option to help decouple 
- observability with logging, metrics, distributed tracing
- circuit breakers, correct timeouts in place
- proper software engineering vs hype driven engineering

* The ugly

* The patron framework

goals of the framework.

- written in go
- boilerplate code, that after a while gets boring, is now gone
- go mod support
- abstract away the
- observability build in
-- structured log, with zerolog, to std out
-- metrics with prometheus
-- distributed tracing with jaeger's opentracing implementation
- health check via '/health'
- service info available via '/info'
- pprof available via '/debug/pprof'
- version support
- clients with distr. tracing available
-- http
-- sql
-- amqp
-- kafka
- sane defaults
-- logging info level
-- disabled distibuted tracing
-- metrics available '/metrics'
- patron cli with
-- docker support
-- packages template
-- go module and vendoring
-- git repo creation

keeping it simple and small????

what's next?

- circuit breaker integration in clients
- consumer groups in kafka 
- security
- protobuf support
- maybe throttling
- check out https://github.com/mantzas/patron/issues

if you want do contribute pls do.

* Show time

- patron cli
- HTTP component and client
- Kafka component and client
- RabbitMQ component and client